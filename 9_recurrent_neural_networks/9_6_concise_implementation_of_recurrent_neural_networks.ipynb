{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(d2l.Module):  # @save\n",
    "    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(d2l.RNNLMScratch):  # @save\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "\n",
    "    def output_layer(self, hiddens):\n",
    "        return self.linear(hiddens).swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为`RNNLM`的类，它继承了`d2l.RNNLMScratch`类，并使用高级API实现了基于RNN的语言模型。以下是逐步解释：\n",
    "\n",
    "1. `init_params(self)`: 这个方法用于初始化模型的参数。在这个实现中，我们使用PyTorch的`nn.LazyLinear`层代替手动定义的权重矩阵`self.W_hq`和偏置向量`self.b_q`。`nn.LazyLinear`层是一个线性层，它会在第一次前向传播时根据输入的形状自动确定权重矩阵的形状。在这里，我们将词汇表的大小作为输出维度传递给`nn.LazyLinear`。\n",
    "\n",
    "2. `output_layer(self, hiddens)`: 这个方法接收RNN的隐藏状态，并通过一个线性层将其转换为最终的输出。与`RNNLMScratch`类中的实现不同，这里我们直接使用`self.linear`（一个`nn.LazyLinear`层）来实现线性变换。我们将隐藏状态`hiddens`输入到`self.linear`中，得到输出张量。然后，我们使用`swapaxes`方法交换输出张量的第0轴和第1轴，使其形状与`RNNLMScratch`类中的实现一致。\n",
    "\n",
    "总的来说，这个类实现了一个基于RNN的语言模型，使用高级API替换了手动定义的权重矩阵和偏置向量。这使得代码更简洁，易于维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'it hashsshsshsshsshsshsshs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/home/michaelpei/code/dive_into_deep_learning/9_recurrent_neural_networks/9_6_concise_implementation_of_recurrent_neural_networks.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B9.135.143.180/data/home/michaelpei/code/dive_into_deep_learning/9_recurrent_neural_networks/9_6_concise_implementation_of_recurrent_neural_networks.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, gradient_clip_val\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B9.135.143.180/data/home/michaelpei/code/dive_into_deep_learning/9_recurrent_neural_networks/9_6_concise_implementation_of_recurrent_neural_networks.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/d2l/torch.py:279\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, model, data):\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_data(data)\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_model(model)\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfigure_optimizers()\n\u001b[1;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/d2l/torch.py:332\u001b[0m, in \u001b[0;36mTrainer.prepare_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    330\u001b[0m model\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mxlim \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs]\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgpus:\n\u001b[0;32m--> 332\u001b[0m     model\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpus[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:197\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m--> 197\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    199\u001b[0m     \u001b[39m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/home/michaelpei/code/dive_into_deep_learning/9_recurrent_neural_networks/9_6_concise_implementation_of_recurrent_neural_networks.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B9.135.143.180/data/home/michaelpei/code/dive_into_deep_learning/9_recurrent_neural_networks/9_6_concise_implementation_of_recurrent_neural_networks.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict(\u001b[39m'\u001b[39;49m\u001b[39mit has\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m20\u001b[39;49m, data\u001b[39m.\u001b[39;49mvocab, d2l\u001b[39m.\u001b[39;49mtry_gpu())\n",
      "File \u001b[0;32m/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/d2l/torch.py:797\u001b[0m, in \u001b[0;36mRNNLMScratch.predict\u001b[0;34m(self, prefix, num_preds, vocab, device)\u001b[0m\n\u001b[1;32m    795\u001b[0m state, outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, [vocab[prefix[\u001b[39m0\u001b[39m]]]\n\u001b[1;32m    796\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(prefix) \u001b[39m+\u001b[39m num_preds \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 797\u001b[0m     X \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39;49mtensor([[outputs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]]], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    798\u001b[0m     embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_hot(X)\n\u001b[1;32m    799\u001b[0m     rnn_outputs, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embs, state)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 高级API在深度学习框架中提供了标准RNN的实现。这些库可以帮助你避免浪费时间重新实现标准模型。此外，框架实现通常经过高度优化，与从头开始的实现相比，计算性能会有显著提升。\n",
    "\n",
    "   使用高级API使RNN模型过拟合是可能的。过拟合通常是由于模型过于复杂，以至于它能够记住训练数据中的噪声而不仅仅是信号。为了使模型过拟合，你可以尝试以下方法：\n",
    "   - 增加模型的复杂性，例如增加RNN的层数或隐藏单元数量。\n",
    "   - 减少训练数据量，这样模型更容易记住训练数据中的特定细节。\n",
    "   - 增加训练迭代次数，使模型在训练数据上训练更长时间。\n",
    "   - 减少或取消正则化（如果有的话），如权重衰减或dropout。\n",
    "\n",
    "2. 使用RNN实现第9.1节的自回归模型：\n",
    "\n",
    "   要使用RNN实现自回归模型，你可以参考本节中的`RNNLMScratch`或`RNNLM`类的实现。自回归模型的主要思想是使用过去的观测值来预测未来的观测值。在这里，我们可以将RNN用作自回归模型，其中输入是过去的观测值序列，输出是未来观测值的预测。具体来说，你可以按照以下步骤实现自回归模型：\n",
    "\n",
    "   - 定义一个RNN模型，可以使用标准RNN、GRU或LSTM等不同类型的RNN。\n",
    "   - 初始化模型的权重和偏置。\n",
    "   - 定义前向传播过程，将输入序列传递给RNN，并得到输出序列。\n",
    "   - 使用适当的损失函数（如均方误差）计算预测值与真实值之间的差异。\n",
    "   - 使用梯度下降法优化模型的参数。\n",
    "   - 训练模型，并在验证集上评估性能。\n",
    "\n",
    "   在实现自回归模型时，你可能需要根据具体问题调整模型的架构、损失函数和优化算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

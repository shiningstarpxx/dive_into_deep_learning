{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cpu():  # @save\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def gpu(i=0):  # @save\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    return torch.device(f'cuda:{i}')\n",
    "\n",
    "\n",
    "cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_gpus():  # @save\n",
    "    \"\"\"Get the number of available GPUs.\"\"\"\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "\n",
    "num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  # @save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if num_gpus() >= i + 1:\n",
    "        return gpu(i)\n",
    "    return cpu()\n",
    "\n",
    "\n",
    "def try_all_gpus():  # @save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    return [gpu(i) for i in range(num_gpus())]\n",
    "\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7000, 0.2187, 0.8960],\n",
       "        [0.2203, 0.9702, 0.0400]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(0)) # we only have one GPU\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "Z = X.cuda(0)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7000, 1.2187, 1.8960],\n",
       "        [1.2203, 1.9702, 1.0400]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.cuda(0) is Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9593],\n",
       "        [0.9593]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  # @save\n",
    "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "    self.save_hyperparameters()\n",
    "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  # @save\n",
    "def prepare_batch(self, batch):\n",
    "    if self.gpus:\n",
    "        batch = [a.to(self.gpus[0]) for a in batch]\n",
    "    return batch\n",
    "\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  # @save\n",
    "def prepare_model(self, model):\n",
    "    model.trainer = self\n",
    "    model.board.xlim = [0, self.max_epochs]\n",
    "    if self.gpus:\n",
    "        model.to(self.gpus[0])\n",
    "    self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large matrix multiplication:\n",
      "CPU: 1.1213064193725586 seconds\n",
      "GPU: 0.14332032203674316 seconds\n",
      "Small matrix multiplication:\n",
      "CPU: 0.00016021728515625 seconds\n",
      "GPU: 0.00010704994201660156 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here's an example to demonstrate the difference in speed between CPU and GPU for a large matrix multiplication task and a small matrix multiplication task:\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def measure_time(matrix_size, device):\n",
    "    A = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    B = torch.randn(matrix_size, matrix_size, device=device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    C = torch.matmul(A, B)\n",
    "    torch.cuda.synchronize()  # Ensure synchronization for GPU\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "large_matrix_size = 5000\n",
    "small_matrix_size = 50\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "gpu_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Large matrix multiplication:\")\n",
    "print(\"CPU:\", measure_time(large_matrix_size, cpu_device), \"seconds\")\n",
    "print(\"GPU:\", measure_time(large_matrix_size, gpu_device), \"seconds\")\n",
    "\n",
    "print(\"Small matrix multiplication:\")\n",
    "print(\"CPU:\", measure_time(small_matrix_size, cpu_device), \"seconds\")\n",
    "print(\"GPU:\", measure_time(small_matrix_size, gpu_device), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example to demonstrate the difference in speed between CPU and GPU for a large matrix multiplication task and a small matrix multiplication task:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def measure_time(matrix_size, device):\n",
    "    A = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    B = torch.randn(matrix_size, matrix_size, device=device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    C = torch.matmul(A, B)\n",
    "    torch.cuda.synchronize()  # Ensure synchronization for GPU\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "large_matrix_size = 5000\n",
    "small_matrix_size = 50\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "gpu_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Large matrix multiplication:\")\n",
    "print(\"CPU:\", measure_time(large_matrix_size, cpu_device), \"seconds\")\n",
    "print(\"GPU:\", measure_time(large_matrix_size, gpu_device), \"seconds\")\n",
    "\n",
    "print(\"Small matrix multiplication:\")\n",
    "print(\"CPU:\", measure_time(small_matrix_size, cpu_device), \"seconds\")\n",
    "print(\"GPU:\", measure_time(small_matrix_size, gpu_device), \"seconds\")\n",
    "```\n",
    "\n",
    "For a large matrix multiplication task, you should observe a significant speedup on the GPU compared to the CPU. However, for a small matrix multiplication task, the difference in speed might be less noticeable or even slower on the GPU due to the overhead of GPU computation and data transfer.\n",
    "\n",
    "To read and write model parameters on the GPU, you can use the `to()` method to move the model and its parameters to the GPU, and the `state_dict()` and `load_state_dict()` methods to save and load the parameters. Here's an example:\n",
    "\n",
    "```python\n",
    "# Move the model to the GPU\n",
    "model = model.to(gpu_device)\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"model_parameters.pt\")\n",
    "\n",
    "# Load the model parameters\n",
    "loaded_parameters = torch.load(\"model_parameters.pt\", map_location=gpu_device)\n",
    "model.load_state_dict(loaded_parameters)\n",
    "```\n",
    "\n",
    "Now, let's measure the time it takes to compute 1000 matrix–matrix multiplications and log the Frobenius norm of the output matrix one result at a time, compared to keeping a log on the GPU and transferring only the final result:\n",
    "\n",
    "```python\n",
    "num_matrices = 1000\n",
    "matrix_size = 500\n",
    "\n",
    "A = torch.randn(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "B = torch.randn(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "C = torch.empty(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "\n",
    "# Compute and log the Frobenius norm one result at a time\n",
    "start_time = time.time()\n",
    "for i in range(num_matrices):\n",
    "    C[i] = torch.matmul(A[i], B[i])\n",
    "    frobenius_norm = torch.norm(C[i], p=\"fro\").cpu().item()\n",
    "    # Log the Frobenius norm (here we just print it)\n",
    "    print(frobenius_norm)\n",
    "end_time = time.time()\n",
    "print(\"One result at a time:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Compute and log the Frobenius norm only for the final result\n",
    "start_time = time.time()\n",
    "C = torch.matmul(A, B)\n",
    "frobenius_norms = torch.norm(C, p=\"fro\", dim=(1, 2)).cpu()\n",
    "end_time = time.time()\n",
    "print(\"Final result only:\", end_time - start_time, \"seconds\")\n",
    "```\n",
    "\n",
    "You should observe that keeping a log on the GPU and transferring only the final result is faster than logging the Frobenius norm one result at a time.\n",
    "\n",
    "Finally, let's measure how much time it takes to perform two matrix–matrix multiplications on two GPUs at the same time, compared to computing them in sequence on one GPU:\n",
    "\n",
    "```python\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    # Two GPUs available\n",
    "    gpu1_device = torch.device(\"cuda:0\")\n",
    "    gpu2_device = torch.device(\"cuda:1\")\n",
    "\n",
    "    A1 = torch.randn(matrix_size, matrix_size, device=gpu1_device)\n",
    "    B1 = torch.randn(matrix_size, matrix_size, device=gpu1_device)\n",
    "    A2 = torch.randn(matrix_size, matrix_size, device=gpu2_device)\n",
    "    B2 = torch.randn(matrix_size, matrix_size, device=gpu2_device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    C1 = torch.matmul(A1, B1)\n",
    "    C2 = torch.matmul(A2, B2)\n",
    "    torch.cuda.synchronize()  # Ensure synchronization for both GPUs\n",
    "    end_time = time.time()\n",
    "    print(\"Two GPUs:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "A1 = A1.to(gpu_device)\n",
    "B1 = B1.to(gpu_device)\n",
    "A2 = A2.to(g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抱歉，我的回答被意外截断。请允许我继续上面的答案。\n",
    "\n",
    "接下来，我们将测量在两个GPU上同时执行两个矩阵-矩阵乘法所需的时间，并将其与在一个GPU上顺序计算它们所需的时间进行比较：\n",
    "\n",
    "```python\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    # Two GPUs available\n",
    "    gpu1_device = torch.device(\"cuda:0\")\n",
    "    gpu2_device = torch.device(\"cuda:1\")\n",
    "\n",
    "    A1 = torch.randn(matrix_size, matrix_size, device=gpu1_device)\n",
    "    B1 = torch.randn(matrix_size, matrix_size, device=gpu1_device)\n",
    "    A2 = torch.randn(matrix_size, matrix_size, device=gpu2_device)\n",
    "    B2 = torch.randn(matrix_size, matrix_size, device=gpu2_device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    C1 = torch.matmul(A1, B1)\n",
    "    C2 = torch.matmul(A2, B2)\n",
    "    torch.cuda.synchronize()  # Ensure synchronization for both GPUs\n",
    "    end_time = time.time()\n",
    "    print(\"Two GPUs:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Move all tensors to the same GPU device\n",
    "A1 = A1.to(gpu_device)\n",
    "B1 = B1.to(gpu_device)\n",
    "A2 = A2.to(gpu_device)\n",
    "B2 = B2.to(gpu_device)\n",
    "\n",
    "start_time = time.time()\n",
    "C1 = torch.matmul(A1, B1)\n",
    "C2 = torch.matmul(A2, B2)\n",
    "torch.cuda.synchronize()  # Ensure synchronization for the single GPU\n",
    "end_time = time.time()\n",
    "print(\"One GPU:\", end_time - start_time, \"seconds\")\n",
    "```\n",
    "\n",
    "当在两个GPU上同时执行矩阵-矩阵乘法时，您应该看到几乎线性的扩展，即所需时间大约是在一个GPU上顺序计算所需时间的一半。这是因为两个GPU可以并行处理计算任务，从而显著减少总体计算时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11237.4482421875\n",
      "11218.3046875\n",
      "11169.251953125\n",
      "11191.328125\n",
      "11170.3046875\n",
      "11165.5322265625\n",
      "11158.1552734375\n",
      "11169.115234375\n",
      "11207.3603515625\n",
      "11192.583984375\n",
      "11187.5126953125\n",
      "11166.400390625\n",
      "11162.6376953125\n",
      "11148.673828125\n",
      "11171.458984375\n",
      "11156.2685546875\n",
      "11155.4248046875\n",
      "11148.712890625\n",
      "11142.8369140625\n",
      "11206.29296875\n",
      "11170.2490234375\n",
      "11186.9609375\n",
      "11188.642578125\n",
      "11172.740234375\n",
      "11222.0302734375\n",
      "11143.40234375\n",
      "11186.3515625\n",
      "11173.91015625\n",
      "11210.4951171875\n",
      "11179.443359375\n",
      "11212.7236328125\n",
      "11178.3515625\n",
      "11164.1923828125\n",
      "11174.9697265625\n",
      "11164.837890625\n",
      "11177.880859375\n",
      "11176.1591796875\n",
      "11175.2705078125\n",
      "11179.2724609375\n",
      "11175.5673828125\n",
      "11149.845703125\n",
      "11113.1220703125\n",
      "11207.4951171875\n",
      "11161.19921875\n",
      "11228.517578125\n",
      "11171.904296875\n",
      "11185.544921875\n",
      "11211.798828125\n",
      "11190.5634765625\n",
      "11184.025390625\n",
      "11177.912109375\n",
      "11165.890625\n",
      "11152.3896484375\n",
      "11179.6689453125\n",
      "11204.3876953125\n",
      "11187.2958984375\n",
      "11210.3984375\n",
      "11253.6767578125\n",
      "11182.8818359375\n",
      "11182.7451171875\n",
      "11199.24609375\n",
      "11191.4814453125\n",
      "11130.84375\n",
      "11165.087890625\n",
      "11139.2900390625\n",
      "11160.7421875\n",
      "11170.412109375\n",
      "11190.662109375\n",
      "11196.498046875\n",
      "11176.595703125\n",
      "11204.328125\n",
      "11167.431640625\n",
      "11182.6201171875\n",
      "11172.4794921875\n",
      "11170.2919921875\n",
      "11178.39453125\n",
      "11207.7822265625\n",
      "11190.3173828125\n",
      "11125.5654296875\n",
      "11138.998046875\n",
      "11124.5888671875\n",
      "11198.0791015625\n",
      "11160.16015625\n",
      "11150.5908203125\n",
      "11167.9873046875\n",
      "11206.95703125\n",
      "11166.044921875\n",
      "11178.9677734375\n",
      "11142.8642578125\n",
      "11124.1552734375\n",
      "11189.3193359375\n",
      "11171.1357421875\n",
      "11179.5615234375\n",
      "11167.0166015625\n",
      "11177.9873046875\n",
      "11206.1064453125\n",
      "11170.8935546875\n",
      "11136.810546875\n",
      "11133.783203125\n",
      "11165.1435546875\n",
      "11164.732421875\n",
      "11203.953125\n",
      "11177.5634765625\n",
      "11159.0\n",
      "11185.91796875\n",
      "11188.6025390625\n",
      "11155.7529296875\n",
      "11142.53515625\n",
      "11186.5888671875\n",
      "11173.595703125\n",
      "11196.798828125\n",
      "11157.4814453125\n",
      "11187.8701171875\n",
      "11145.001953125\n",
      "11177.6279296875\n",
      "11176.68359375\n",
      "11166.1005859375\n",
      "11197.2119140625\n",
      "11199.9609375\n",
      "11140.5615234375\n",
      "11177.0439453125\n",
      "11183.2978515625\n",
      "11171.5400390625\n",
      "11145.6943359375\n",
      "11186.208984375\n",
      "11177.5546875\n",
      "11184.681640625\n",
      "11132.8369140625\n",
      "11148.49609375\n",
      "11146.4560546875\n",
      "11183.19921875\n",
      "11181.7626953125\n",
      "11195.951171875\n",
      "11199.048828125\n",
      "11203.6328125\n",
      "11191.296875\n",
      "11175.029296875\n",
      "11178.0263671875\n",
      "11176.58984375\n",
      "11180.7861328125\n",
      "11192.5546875\n",
      "11129.9541015625\n",
      "11168.46484375\n",
      "11212.009765625\n",
      "11173.2587890625\n",
      "11152.5537109375\n",
      "11131.7333984375\n",
      "11187.970703125\n",
      "11176.1669921875\n",
      "11218.525390625\n",
      "11173.4873046875\n",
      "11217.83984375\n",
      "11206.263671875\n",
      "11191.615234375\n",
      "11212.0224609375\n",
      "11174.138671875\n",
      "11205.2548828125\n",
      "11184.0146484375\n",
      "11174.6826171875\n",
      "11178.1845703125\n",
      "11180.302734375\n",
      "11241.1455078125\n",
      "11162.7841796875\n",
      "11160.7880859375\n",
      "11176.9990234375\n",
      "11208.5439453125\n",
      "11161.6904296875\n",
      "11185.982421875\n",
      "11135.6669921875\n",
      "11134.2236328125\n",
      "11201.41796875\n",
      "11146.7314453125\n",
      "11181.0986328125\n",
      "11153.5859375\n",
      "11225.7822265625\n",
      "11191.908203125\n",
      "11159.7138671875\n",
      "11162.119140625\n",
      "11173.3349609375\n",
      "11173.638671875\n",
      "11168.5205078125\n",
      "11143.3310546875\n",
      "11166.884765625\n",
      "11159.744140625\n",
      "11190.5107421875\n",
      "11129.8984375\n",
      "11205.0615234375\n",
      "11170.3564453125\n",
      "11128.630859375\n",
      "11223.7626953125\n",
      "11204.6875\n",
      "11183.0263671875\n",
      "11159.236328125\n",
      "11204.2548828125\n",
      "11136.3271484375\n",
      "11175.2890625\n",
      "11157.587890625\n",
      "11134.296875\n",
      "11179.0693359375\n",
      "11202.6611328125\n",
      "11175.603515625\n",
      "11182.8251953125\n",
      "11202.2470703125\n",
      "11218.67578125\n",
      "11145.1572265625\n",
      "11199.6005859375\n",
      "11215.65234375\n",
      "11171.2412109375\n",
      "11135.5029296875\n",
      "11182.71484375\n",
      "11177.056640625\n",
      "11167.125\n",
      "11193.427734375\n",
      "11206.458984375\n",
      "11211.3037109375\n",
      "11165.3037109375\n",
      "11207.197265625\n",
      "11200.607421875\n",
      "11137.609375\n",
      "11197.458984375\n",
      "11191.791015625\n",
      "11126.822265625\n",
      "11169.50390625\n",
      "11158.39453125\n",
      "11215.6162109375\n",
      "11192.8369140625\n",
      "11174.712890625\n",
      "11217.697265625\n",
      "11201.0732421875\n",
      "11210.349609375\n",
      "11195.5869140625\n",
      "11164.66796875\n",
      "11183.943359375\n",
      "11160.1494140625\n",
      "11145.994140625\n",
      "11185.912109375\n",
      "11184.619140625\n",
      "11182.1728515625\n",
      "11213.0712890625\n",
      "11178.24609375\n",
      "11182.552734375\n",
      "11196.35546875\n",
      "11216.6357421875\n",
      "11171.53515625\n",
      "11162.603515625\n",
      "11167.4150390625\n",
      "11148.7890625\n",
      "11167.3134765625\n",
      "11175.974609375\n",
      "11135.7041015625\n",
      "11215.0498046875\n",
      "11136.7451171875\n",
      "11196.625\n",
      "11146.1669921875\n",
      "11169.5693359375\n",
      "11228.23046875\n",
      "11220.2734375\n",
      "11136.642578125\n",
      "11169.69140625\n",
      "11218.6259765625\n",
      "11136.5205078125\n",
      "11188.3466796875\n",
      "11207.455078125\n",
      "11173.2734375\n",
      "11176.13671875\n",
      "11204.24609375\n",
      "11216.599609375\n",
      "11190.5419921875\n",
      "11185.16796875\n",
      "11160.63671875\n",
      "11186.1015625\n",
      "11158.509765625\n",
      "11171.1640625\n",
      "11143.6640625\n",
      "11210.2607421875\n",
      "11165.4140625\n",
      "11173.302734375\n",
      "11176.7861328125\n",
      "11162.9443359375\n",
      "11156.9443359375\n",
      "11146.6181640625\n",
      "11164.7578125\n",
      "11207.1474609375\n",
      "11186.50390625\n",
      "11217.5576171875\n",
      "11187.3583984375\n",
      "11186.658203125\n",
      "11174.84375\n",
      "11190.5400390625\n",
      "11171.1162109375\n",
      "11188.93359375\n",
      "11180.3916015625\n",
      "11224.9052734375\n",
      "11119.6025390625\n",
      "11203.0849609375\n",
      "11182.8115234375\n",
      "11204.1572265625\n",
      "11225.5166015625\n",
      "11148.728515625\n",
      "11203.791015625\n",
      "11219.8935546875\n",
      "11185.5517578125\n",
      "11193.390625\n",
      "11182.9521484375\n",
      "11212.451171875\n",
      "11208.3017578125\n",
      "11169.7548828125\n",
      "11184.013671875\n",
      "11147.15625\n",
      "11168.4111328125\n",
      "11178.8271484375\n",
      "11179.8564453125\n",
      "11153.111328125\n",
      "11156.53125\n",
      "11117.7822265625\n",
      "11165.203125\n",
      "11213.2548828125\n",
      "11158.2060546875\n",
      "11194.8583984375\n",
      "11176.2724609375\n",
      "11151.6533203125\n",
      "11179.8642578125\n",
      "11200.8037109375\n",
      "11129.751953125\n",
      "11156.8662109375\n",
      "11174.734375\n",
      "11176.9384765625\n",
      "11162.29296875\n",
      "11180.8701171875\n",
      "11155.9130859375\n",
      "11167.2890625\n",
      "11191.4296875\n",
      "11167.9970703125\n",
      "11144.44921875\n",
      "11155.95703125\n",
      "11172.994140625\n",
      "11180.1943359375\n",
      "11186.0126953125\n",
      "11169.9111328125\n",
      "11161.3603515625\n",
      "11173.92578125\n",
      "11179.173828125\n",
      "11157.390625\n",
      "11213.23046875\n",
      "11192.7607421875\n",
      "11195.6689453125\n",
      "11174.2529296875\n",
      "11147.4404296875\n",
      "11192.4697265625\n",
      "11163.16015625\n",
      "11212.1552734375\n",
      "11238.513671875\n",
      "11194.5087890625\n",
      "11175.5625\n",
      "11199.5517578125\n",
      "11189.44140625\n",
      "11146.404296875\n",
      "11185.9150390625\n",
      "11160.3359375\n",
      "11162.2236328125\n",
      "11216.4091796875\n",
      "11197.376953125\n",
      "11215.3623046875\n",
      "11156.609375\n",
      "11196.9892578125\n",
      "11238.1552734375\n",
      "11186.8798828125\n",
      "11166.51953125\n",
      "11200.6337890625\n",
      "11183.1435546875\n",
      "11162.5673828125\n",
      "11163.3955078125\n",
      "11176.794921875\n",
      "11177.833984375\n",
      "11187.9384765625\n",
      "11198.6005859375\n",
      "11174.201171875\n",
      "11157.439453125\n",
      "11162.873046875\n",
      "11192.0283203125\n",
      "11192.7939453125\n",
      "11165.193359375\n",
      "11223.2412109375\n",
      "11218.9306640625\n",
      "11157.4658203125\n",
      "11236.0712890625\n",
      "11177.53515625\n",
      "11160.1201171875\n",
      "11206.25\n",
      "11133.42578125\n",
      "11173.1181640625\n",
      "11186.0205078125\n",
      "11146.068359375\n",
      "11184.927734375\n",
      "11177.5361328125\n",
      "11189.0830078125\n",
      "11134.7734375\n",
      "11172.05859375\n",
      "11195.6318359375\n",
      "11142.9990234375\n",
      "11209.3837890625\n",
      "11151.6240234375\n",
      "11195.6572265625\n",
      "11153.6025390625\n",
      "11201.7236328125\n",
      "11177.3603515625\n",
      "11244.734375\n",
      "11177.09375\n",
      "11141.5771484375\n",
      "11187.845703125\n",
      "11170.6748046875\n",
      "11117.3994140625\n",
      "11167.36328125\n",
      "11204.28515625\n",
      "11207.3369140625\n",
      "11186.130859375\n",
      "11188.552734375\n",
      "11145.5009765625\n",
      "11172.56640625\n",
      "11180.2236328125\n",
      "11218.8525390625\n",
      "11168.12109375\n",
      "11168.375\n",
      "11177.01171875\n",
      "11228.9921875\n",
      "11180.3310546875\n",
      "11135.0244140625\n",
      "11212.919921875\n",
      "11144.197265625\n",
      "11179.7646484375\n",
      "11190.921875\n",
      "11131.4638671875\n",
      "11171.6748046875\n",
      "11223.03125\n",
      "11188.9033203125\n",
      "11240.54296875\n",
      "11222.376953125\n",
      "11153.4130859375\n",
      "11241.771484375\n",
      "11178.6767578125\n",
      "11214.72265625\n",
      "11260.3203125\n",
      "11182.0703125\n",
      "11237.1767578125\n",
      "11199.515625\n",
      "11184.2880859375\n",
      "11175.2578125\n",
      "11163.341796875\n",
      "11209.439453125\n",
      "11154.5810546875\n",
      "11202.4619140625\n",
      "11170.25390625\n",
      "11177.498046875\n",
      "11167.9033203125\n",
      "11218.92578125\n",
      "11156.01171875\n",
      "11193.9541015625\n",
      "11179.427734375\n",
      "11183.3017578125\n",
      "11178.5498046875\n",
      "11197.333984375\n",
      "11167.6181640625\n",
      "11217.0908203125\n",
      "11178.9658203125\n",
      "11188.482421875\n",
      "11169.90625\n",
      "11150.8369140625\n",
      "11198.85546875\n",
      "11175.5224609375\n",
      "11168.7890625\n",
      "11170.419921875\n",
      "11176.7451171875\n",
      "11200.3916015625\n",
      "11160.6484375\n",
      "11134.439453125\n",
      "11194.888671875\n",
      "11158.6875\n",
      "11233.689453125\n",
      "11176.0458984375\n",
      "11161.7314453125\n",
      "11200.2138671875\n",
      "11195.0009765625\n",
      "11212.783203125\n",
      "11195.3251953125\n",
      "11177.134765625\n",
      "11130.630859375\n",
      "11161.34765625\n",
      "11213.044921875\n",
      "11235.220703125\n",
      "11137.2431640625\n",
      "11204.8193359375\n",
      "11176.0244140625\n",
      "11195.505859375\n",
      "11181.767578125\n",
      "11226.35546875\n",
      "11161.30859375\n",
      "11177.91796875\n",
      "11220.2431640625\n",
      "11208.001953125\n",
      "11261.8681640625\n",
      "11236.2109375\n",
      "11143.7890625\n",
      "11198.1953125\n",
      "11166.6875\n",
      "11181.5751953125\n",
      "11136.3974609375\n",
      "11198.41015625\n",
      "11152.01171875\n",
      "11189.619140625\n",
      "11174.99609375\n",
      "11172.9921875\n",
      "11208.845703125\n",
      "11164.345703125\n",
      "11183.1787109375\n",
      "11126.853515625\n",
      "11179.8984375\n",
      "11192.1064453125\n",
      "11203.39453125\n",
      "11188.0712890625\n",
      "11106.0302734375\n",
      "11185.7275390625\n",
      "11077.8056640625\n",
      "11181.50390625\n",
      "11138.1015625\n",
      "11177.44921875\n",
      "11181.55859375\n",
      "11218.0791015625\n",
      "11195.6865234375\n",
      "11163.041015625\n",
      "11162.52734375\n",
      "11168.1328125\n",
      "11223.03125\n",
      "11173.103515625\n",
      "11220.1083984375\n",
      "11189.1435546875\n",
      "11214.83203125\n",
      "11216.8583984375\n",
      "11181.8505859375\n",
      "11154.677734375\n",
      "11209.958984375\n",
      "11199.8369140625\n",
      "11166.2529296875\n",
      "11180.2958984375\n",
      "11161.6259765625\n",
      "11172.0224609375\n",
      "11123.5\n",
      "11169.5009765625\n",
      "11190.224609375\n",
      "11190.2314453125\n",
      "11203.9091796875\n",
      "11179.73046875\n",
      "11214.087890625\n",
      "11205.216796875\n",
      "11248.279296875\n",
      "11199.9345703125\n",
      "11169.2021484375\n",
      "11163.4990234375\n",
      "11201.46875\n",
      "11177.572265625\n",
      "11153.7841796875\n",
      "11161.3916015625\n",
      "11142.2685546875\n",
      "11195.78125\n",
      "11200.484375\n",
      "11189.9833984375\n",
      "11154.37890625\n",
      "11178.1181640625\n",
      "11172.4990234375\n",
      "11193.9501953125\n",
      "11181.798828125\n",
      "11190.59375\n",
      "11149.2998046875\n",
      "11091.6337890625\n",
      "11156.7080078125\n",
      "11142.640625\n",
      "11165.689453125\n",
      "11180.6689453125\n",
      "11186.1748046875\n",
      "11174.3759765625\n",
      "11204.1904296875\n",
      "11154.70703125\n",
      "11178.3671875\n",
      "11128.7919921875\n",
      "11172.0126953125\n",
      "11160.8408203125\n",
      "11173.2890625\n",
      "11219.9697265625\n",
      "11175.0966796875\n",
      "11186.919921875\n",
      "11172.0712890625\n",
      "11189.3291015625\n",
      "11179.0859375\n",
      "11187.4287109375\n",
      "11189.48046875\n",
      "11159.3759765625\n",
      "11225.7265625\n",
      "11162.416015625\n",
      "11167.8701171875\n",
      "11185.115234375\n",
      "11193.9345703125\n",
      "11146.8251953125\n",
      "11166.66796875\n",
      "11202.70703125\n",
      "11151.7880859375\n",
      "11249.8447265625\n",
      "11192.771484375\n",
      "11184.1298828125\n",
      "11192.4794921875\n",
      "11212.75390625\n",
      "11176.0751953125\n",
      "11248.7333984375\n",
      "11186.541015625\n",
      "11195.5048828125\n",
      "11187.1572265625\n",
      "11187.2822265625\n",
      "11210.91796875\n",
      "11139.7666015625\n",
      "11213.5078125\n",
      "11166.181640625\n",
      "11185.232421875\n",
      "11175.73046875\n",
      "11187.51953125\n",
      "11187.91015625\n",
      "11189.0107421875\n",
      "11208.4326171875\n",
      "11182.6181640625\n",
      "11156.06640625\n",
      "11167.880859375\n",
      "11134.0244140625\n",
      "11098.7841796875\n",
      "11167.7490234375\n",
      "11152.26171875\n",
      "11159.23046875\n",
      "11182.1689453125\n",
      "11168.765625\n",
      "11196.6884765625\n",
      "11166.04296875\n",
      "11162.888671875\n",
      "11232.4599609375\n",
      "11131.9951171875\n",
      "11167.4013671875\n",
      "11130.994140625\n",
      "11170.7529296875\n",
      "11205.3408203125\n",
      "11142.56640625\n",
      "11214.6806640625\n",
      "11216.513671875\n",
      "11213.5068359375\n",
      "11159.8017578125\n",
      "11155.32421875\n",
      "11184.4169921875\n",
      "11123.8388671875\n",
      "11190.6181640625\n",
      "11187.455078125\n",
      "11187.8798828125\n",
      "11170.2783203125\n",
      "11185.34375\n",
      "11185.8359375\n",
      "11216.615234375\n",
      "11177.73046875\n",
      "11116.556640625\n",
      "11185.6513671875\n",
      "11142.6240234375\n",
      "11171.07421875\n",
      "11179.3115234375\n",
      "11187.859375\n",
      "11160.8876953125\n",
      "11198.7724609375\n",
      "11207.2607421875\n",
      "11189.33203125\n",
      "11234.9716796875\n",
      "11146.25390625\n",
      "11189.439453125\n",
      "11189.8505859375\n",
      "11177.6943359375\n",
      "11185.380859375\n",
      "11190.5791015625\n",
      "11230.068359375\n",
      "11134.9189453125\n",
      "11189.263671875\n",
      "11171.3740234375\n",
      "11183.677734375\n",
      "11131.0380859375\n",
      "11171.5380859375\n",
      "11141.36328125\n",
      "11182.3701171875\n",
      "11153.3056640625\n",
      "11162.779296875\n",
      "11178.345703125\n",
      "11137.9609375\n",
      "11168.1171875\n",
      "11184.4296875\n",
      "11204.1640625\n",
      "11157.80078125\n",
      "11110.5888671875\n",
      "11149.2666015625\n",
      "11160.5771484375\n",
      "11156.689453125\n",
      "11194.791015625\n",
      "11121.1318359375\n",
      "11153.373046875\n",
      "11182.41796875\n",
      "11157.361328125\n",
      "11136.8955078125\n",
      "11208.3173828125\n",
      "11187.9052734375\n",
      "11181.5810546875\n",
      "11215.0068359375\n",
      "11169.1552734375\n",
      "11171.1513671875\n",
      "11206.462890625\n",
      "11155.22265625\n",
      "11187.0751953125\n",
      "11203.96875\n",
      "11195.623046875\n",
      "11173.365234375\n",
      "11150.6142578125\n",
      "11145.0068359375\n",
      "11152.603515625\n",
      "11163.853515625\n",
      "11184.0439453125\n",
      "11174.109375\n",
      "11197.8681640625\n",
      "11185.9208984375\n",
      "11164.423828125\n",
      "11186.2158203125\n",
      "11158.0341796875\n",
      "11205.8935546875\n",
      "11165.11328125\n",
      "11215.0751953125\n",
      "11193.0849609375\n",
      "11141.0888671875\n",
      "11175.0791015625\n",
      "11187.6474609375\n",
      "11189.0537109375\n",
      "11187.2919921875\n",
      "11196.5791015625\n",
      "11198.802734375\n",
      "11204.9345703125\n",
      "11134.939453125\n",
      "11117.326171875\n",
      "11171.529296875\n",
      "11199.255859375\n",
      "11225.78515625\n",
      "11165.849609375\n",
      "11201.140625\n",
      "11144.8544921875\n",
      "11146.767578125\n",
      "11192.55078125\n",
      "11157.1328125\n",
      "11196.642578125\n",
      "11209.310546875\n",
      "11143.6025390625\n",
      "11167.9189453125\n",
      "11177.3828125\n",
      "11148.90625\n",
      "11206.9111328125\n",
      "11161.28125\n",
      "11186.5146484375\n",
      "11150.849609375\n",
      "11167.939453125\n",
      "11234.9541015625\n",
      "11199.763671875\n",
      "11206.837890625\n",
      "11217.7734375\n",
      "11211.24609375\n",
      "11151.435546875\n",
      "11192.6904296875\n",
      "11198.4462890625\n",
      "11186.93359375\n",
      "11166.9306640625\n",
      "11130.533203125\n",
      "11199.1103515625\n",
      "11169.5498046875\n",
      "11173.0185546875\n",
      "11175.970703125\n",
      "11203.2626953125\n",
      "11192.0361328125\n",
      "11180.791015625\n",
      "11136.673828125\n",
      "11167.3720703125\n",
      "11156.0224609375\n",
      "11217.8115234375\n",
      "11132.2490234375\n",
      "11164.1982421875\n",
      "11186.8525390625\n",
      "11207.12890625\n",
      "11182.970703125\n",
      "11241.6279296875\n",
      "11210.5888671875\n",
      "11173.3193359375\n",
      "11203.775390625\n",
      "11154.232421875\n",
      "11155.228515625\n",
      "11202.0615234375\n",
      "11193.609375\n",
      "11191.1875\n",
      "11198.6943359375\n",
      "11165.7958984375\n",
      "11166.6474609375\n",
      "11177.8828125\n",
      "11194.693359375\n",
      "11204.3193359375\n",
      "11189.8857421875\n",
      "11204.5615234375\n",
      "11144.0244140625\n",
      "11177.3740234375\n",
      "11155.0302734375\n",
      "11201.1904296875\n",
      "11186.5107421875\n",
      "11176.9833984375\n",
      "11171.48046875\n",
      "11222.12890625\n",
      "11142.9658203125\n",
      "11137.990234375\n",
      "11122.2998046875\n",
      "11160.6396484375\n",
      "11200.970703125\n",
      "11185.0927734375\n",
      "11198.35546875\n",
      "11193.8525390625\n",
      "11185.0888671875\n",
      "11170.7001953125\n",
      "11198.3779296875\n",
      "11188.2421875\n",
      "11192.0419921875\n",
      "11153.2158203125\n",
      "11209.791015625\n",
      "11133.1572265625\n",
      "11188.3115234375\n",
      "11236.5224609375\n",
      "11207.7470703125\n",
      "11198.7421875\n",
      "11195.43359375\n",
      "11165.939453125\n",
      "11213.4208984375\n",
      "11186.05078125\n",
      "11182.65234375\n",
      "11199.1044921875\n",
      "11158.744140625\n",
      "11187.337890625\n",
      "11172.2861328125\n",
      "11230.53515625\n",
      "11199.49609375\n",
      "11200.615234375\n",
      "11111.537109375\n",
      "11195.57421875\n",
      "11208.8173828125\n",
      "11204.7119140625\n",
      "11171.1953125\n",
      "11186.966796875\n",
      "11235.39453125\n",
      "11172.216796875\n",
      "11185.828125\n",
      "11122.7099609375\n",
      "11191.76953125\n",
      "11160.8134765625\n",
      "11121.0830078125\n",
      "11164.109375\n",
      "11129.50390625\n",
      "11178.25390625\n",
      "11168.3623046875\n",
      "11192.142578125\n",
      "11200.5830078125\n",
      "11166.806640625\n",
      "11153.4912109375\n",
      "11223.8583984375\n",
      "11167.9599609375\n",
      "11143.0302734375\n",
      "11208.3837890625\n",
      "11171.7978515625\n",
      "11153.4736328125\n",
      "11198.052734375\n",
      "11154.5732421875\n",
      "11196.1640625\n",
      "11196.560546875\n",
      "11177.5986328125\n",
      "11175.9833984375\n",
      "11160.24609375\n",
      "11156.06640625\n",
      "11138.7568359375\n",
      "11213.2353515625\n",
      "11141.908203125\n",
      "11195.3330078125\n",
      "11233.90625\n",
      "11187.9921875\n",
      "11223.15625\n",
      "11207.0009765625\n",
      "11211.2275390625\n",
      "11235.69140625\n",
      "11177.0546875\n",
      "11171.5419921875\n",
      "11143.90625\n",
      "11166.541015625\n",
      "11210.9716796875\n",
      "11147.189453125\n",
      "11203.9296875\n",
      "11217.77734375\n",
      "11159.857421875\n",
      "11175.8466796875\n",
      "11156.60546875\n",
      "11166.5224609375\n",
      "11199.623046875\n",
      "11152.5810546875\n",
      "11155.55078125\n",
      "11150.34375\n",
      "11138.900390625\n",
      "11121.158203125\n",
      "11199.125\n",
      "11177.6845703125\n",
      "11162.09375\n",
      "11147.763671875\n",
      "11159.541015625\n",
      "11165.271484375\n",
      "11156.142578125\n",
      "11171.2421875\n",
      "11209.3447265625\n",
      "11188.1806640625\n",
      "11224.8349609375\n",
      "11158.96484375\n",
      "11129.779296875\n",
      "11183.0546875\n",
      "11131.630859375\n",
      "11199.0029296875\n",
      "11213.4736328125\n",
      "11241.53125\n",
      "11204.2197265625\n",
      "11197.7451171875\n",
      "11259.66015625\n",
      "11150.8544921875\n",
      "11173.7607421875\n",
      "11157.51171875\n",
      "11162.2646484375\n",
      "11151.109375\n",
      "11197.1689453125\n",
      "11157.9423828125\n",
      "11205.0283203125\n",
      "11200.4140625\n",
      "11162.244140625\n",
      "11122.5830078125\n",
      "11225.5048828125\n",
      "11201.556640625\n",
      "11225.30859375\n",
      "11180.25\n",
      "11221.1328125\n",
      "11180.70703125\n",
      "11215.88671875\n",
      "11162.048828125\n",
      "11213.8525390625\n",
      "11187.71484375\n",
      "11183.26171875\n",
      "11194.3115234375\n",
      "11128.01171875\n",
      "11182.6533203125\n",
      "11201.015625\n",
      "11135.232421875\n",
      "11143.3662109375\n",
      "11175.44140625\n",
      "11195.4580078125\n",
      "11243.32421875\n",
      "11147.916015625\n",
      "11219.61328125\n",
      "11232.9931640625\n",
      "11197.2900390625\n",
      "11174.5576171875\n",
      "11210.3505859375\n",
      "11182.01171875\n",
      "11146.166015625\n",
      "11192.9658203125\n",
      "11200.9150390625\n",
      "11222.0419921875\n",
      "11194.45703125\n",
      "11178.8994140625\n",
      "11153.357421875\n",
      "11166.248046875\n",
      "11176.5869140625\n",
      "11178.033203125\n",
      "11210.4755859375\n",
      "11163.3154296875\n",
      "11205.244140625\n",
      "11207.8876953125\n",
      "11197.80078125\n",
      "11238.0947265625\n",
      "11176.232421875\n",
      "11191.6572265625\n",
      "11111.4560546875\n",
      "11161.126953125\n",
      "11205.0986328125\n",
      "11171.615234375\n",
      "11231.841796875\n",
      "11178.357421875\n",
      "11136.5458984375\n",
      "11186.69921875\n",
      "11144.9052734375\n",
      "11161.5107421875\n",
      "11162.7705078125\n",
      "11178.5947265625\n",
      "11210.2626953125\n",
      "11192.75\n",
      "11221.5244140625\n",
      "One result at a time: 0.3176858425140381 seconds\n",
      "Final result only: 0.13779997825622559 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here's an example to demonstrate the difference in speed between CPU and GPU for a large matrix multiplication task and a small matrix multiplication task:\n",
    "import torch\n",
    "import time\n",
    "\n",
    "num_matrices = 1000\n",
    "matrix_size = 500\n",
    "\n",
    "A = torch.randn(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "B = torch.randn(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "C = torch.empty(num_matrices, matrix_size, matrix_size, device=gpu_device)\n",
    "\n",
    "# Compute and log the Frobenius norm one result at a time\n",
    "start_time = time.time()\n",
    "for i in range(num_matrices):\n",
    "    C[i] = torch.matmul(A[i], B[i])\n",
    "    frobenius_norm = torch.norm(C[i], p=\"fro\").cpu().item()\n",
    "    # Log the Frobenius norm (here we just print it)\n",
    "    print(frobenius_norm)\n",
    "end_time = time.time()\n",
    "print(\"One result at a time:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Compute and log the Frobenius norm only for the final result\n",
    "start_time = time.time()\n",
    "C = torch.matmul(A, B)\n",
    "frobenius_norms = torch.norm(C, p=\"fro\", dim=(1, 2)).cpu()\n",
    "end_time = time.time()\n",
    "print(\"Final result only:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

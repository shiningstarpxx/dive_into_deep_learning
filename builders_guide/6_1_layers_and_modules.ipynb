{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that a single neuron \n",
    "\n",
    "(i) takes some set of inputs;  \n",
    "\n",
    "(ii) generates a corresponding scalar output; and  \n",
    "\n",
    "(iii) has a set of associated parameters that can be updated to optimize some objective function of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like individual neurons, layers \n",
    "\n",
    "(i) take a set of inputs, \n",
    "\n",
    "(ii) generate corresponding outputs, and \n",
    "\n",
    "(iii) are described by a set of tunable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire model takes in raw inputs (the features), \n",
    "\n",
    "generates outputs (the predictions), \n",
    "\n",
    "and possesses parameters (the combined parameters from all constituent layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you might think that neurons, layers, and models give us enough abstractions to go about our business, \n",
    "\n",
    "it turns out that we often find it convenient to speak about components that are larger than an individual layer but smaller than the entire model.\n",
    "\n",
    "![](https://d2l.ai/_images/blocks.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we briefly summarize the basic functionality that each module must provide:\n",
    "\n",
    "1. Ingest input data as arguments to its forward propagation method.\n",
    "\n",
    "2. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "\n",
    "3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically.\n",
    "\n",
    "4. Store and provide access to those parameters necessary for executing the forward propagation computation.\n",
    "\n",
    "5. Initialize model parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our own simplified MySequential, we just need to define two key methods:\n",
    "\n",
    "1. A method for appending modules one by one to a list.\n",
    "\n",
    "2. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1621, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3452, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you change MySequential to store modules in a Python list:\n",
    "\n",
    "If you change MySequential to store modules in a Python list, the main issue you might encounter is that the parameters of the modules in the list might not be registered correctly in the parent module (MySequential). This is because PyTorch's nn.Module tracks the parameters of its submodules only when they are added as attributes. When using a Python list, the submodules are not added as attributes, so their parameters might not be recognized by the parent module.\n",
    "\n",
    "To fix this issue, you can use `nn.ModuleList` instead of a Python list. `nn.ModuleList` is specifically designed to store a list of PyTorch modules and handles the registration of their parameters correctly.\n",
    "\n",
    "2. Implement a parallel module:\n",
    "\n",
    "Here's an implementation of a parallel module that takes two modules as arguments and returns the concatenated output of both networks in the forward propagation:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelModule(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.net1(x)\n",
    "        out2 = self.net2(x)\n",
    "        return torch.cat((out1, out2), dim=1)\n",
    "```\n",
    "\n",
    "3. Factory function to generate multiple instances of the same module and build a larger network:\n",
    "\n",
    "Here's a factory function that generates multiple instances of the same module and builds a larger network from it:\n",
    "\n",
    "```python\n",
    "def create_networks(module_class, num_instances, *args, **kwargs):\n",
    "    networks = [module_class(*args, **kwargs) for _ in range(num_instances)]\n",
    "    return networks\n",
    "\n",
    "def build_large_network(networks):\n",
    "    return nn.Sequential(*networks)\n",
    "\n",
    "# Example usage:\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "num_instances = 3\n",
    "networks = create_networks(MyNetwork, num_instances)\n",
    "large_network = build_large_network(networks)\n",
    "```\n",
    "\n",
    "In the example above, the `create_networks` function generates multiple instances of the same module, and the `build_large_network` function combines them into a larger network using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抱歉，我误解了您的问题。要实现并行执行，您可以使用`torch.nn.parallel`模块中的`DataParallel`类。这允许您在多个GPU上并行运行网络，从而加速计算。以下是一个使用`DataParallel`实现并行执行的示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelModule(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.net1(x)\n",
    "        out2 = self.net2(x)\n",
    "        return torch.cat((out1, out2), dim=1)\n",
    "\n",
    "# Example networks\n",
    "net1 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 5)\n",
    ")\n",
    "\n",
    "# Create the parallel module\n",
    "parallel_module = ParallelModule(net1, net2)\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    parallel_module = nn.DataParallel(parallel_module)\n",
    "\n",
    "# Move the parallel module to the available device (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "parallel_module.to(device)\n",
    "\n",
    "# Create input data and move it to the available device\n",
    "input_data = torch.randn(32, 10).to(device)\n",
    "\n",
    "# Forward pass through the parallel module\n",
    "output = parallel_module(input_data)\n",
    "```\n",
    "\n",
    "在这个示例中，我们创建了一个`ParallelModule`，它接受两个子网络`net1`和`net2`。然后，我们使用`nn.DataParallel`类将`ParallelModule`并行化。如果有多个GPU可用，`nn.DataParallel`会自动在这些GPU上分布计算。\n",
    "\n",
    "需要注意的是，`DataParallel`实际上是在训练过程中将输入数据拆分成多个较小的批次，并将这些批次分发到多个GPU上。这样，每个GPU都可以同时处理一部分数据，从而实现并行计算。然后，`DataParallel`会将各个GPU上的输出结果聚合起来，形成最终输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ParallelModule(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.net1(x)\n",
    "        out2 = self.net2(x)\n",
    "        return torch.cat((out1, out2), dim=1)\n",
    "\n",
    "\n",
    "# Example networks\n",
    "net1 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 5)\n",
    ")\n",
    "\n",
    "# Create the parallel module\n",
    "parallel_module = ParallelModule(net1, net2)\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    parallel_module = nn.DataParallel(parallel_module)\n",
    "\n",
    "# Move the parallel module to the available device (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "parallel_module.to(device)\n",
    "\n",
    "# Create input data and move it to the available device\n",
    "input_data = torch.randn(32, 10).to(device)\n",
    "\n",
    "# Forward pass through the parallel module\n",
    "output = parallel_module(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have discussed how to process data and how to build, train, and test deep learning models. \n",
    "# However, at some point we will hopefully be happy enough with the learned models that we will want to \n",
    "# save the results for later use in various contexts (perhaps even to make predictions in deployment). \n",
    "# Additionally, when running a long training process, the best practice is to periodically save intermediate results (checkpointing) \n",
    "# to ensure that we do not lose several days’ worth of computation if we trip over the power cord of our server. \n",
    "# Thus it is time to learn how to load and store both individual weight vectors and entire models. This section addresses both issues.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For individual tensors, we can directly invoke the load and save functions to read and write them respectively. \n",
    "# Both functions require that we supply a name, and save requires as input the variable to be saved.\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can store a list of tensors and read them back into memory.\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even write and read a dictionary that maps from strings to tensors. This is convenient when we want to read or write all the weights in a model.\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Saving individual weight vectors (or other tensors) is useful, \n",
    "# but it gets very tedious if we want to save (and later load) an entire model. \n",
    "# After all, we might have hundreds of parameter groups sprinkled throughout. \n",
    "# For this reason the deep learning framework provides built-in functionalities to load and save entire networks. \n",
    "# An important detail to note is that this saves model parameters and not the entire model. \n",
    "# For example, if we have a 3-layer MLP, we need to specify the architecture separately. \n",
    "# The reason for this is that the models themselves can contain arbitrary code, \n",
    "# hence they cannot be serialized as naturally. Thus, in order to reinstate a model, \n",
    "# we need to generate the architecture in code and then load the parameters from disk. Let’s start with our familiar ML\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Practical benefits of storing model parameters:\n",
    "\n",
    "Even if there is no need to deploy trained models to a different device, storing model parameters has several practical benefits:\n",
    "\n",
    "- **Model checkpointing**: During the training process, you can save intermediate model parameters as checkpoints. This allows you to resume training from the last checkpoint if the training process is interrupted or if you want to fine-tune the model with different hyperparameters.\n",
    "- **Model evaluation**: Storing model parameters allows you to evaluate the model's performance on different datasets or tasks without retraining the model each time.\n",
    "- **Transfer learning**: By storing model parameters, you can use pre-trained models as a starting point for training new models on related tasks or datasets. This can save a significant amount of training time and computational resources.\n",
    "- **Model ensembling**: Storing model parameters allows you to combine multiple models (e.g., trained with different hyperparameters or initializations) to create an ensemble model, which can improve performance and generalization.\n",
    "- **Model versioning**: Saving model parameters allows you to keep track of different versions of the model during development, making it easier to compare their performance and revert to a previous version if needed.\n",
    "\n",
    "2. Reusing parts of a network in a new network with a different architecture:\n",
    "\n",
    "To reuse parts of a network in a new network with a different architecture, you can simply create a new network that includes the desired layers from the previous network. Here's an example of how to use the first two layers from a previous network in a new network:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class OldNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NewNetwork(nn.Module):\n",
    "    def __init__(self, old_network):\n",
    "        super().__init__()\n",
    "        self.first_two_layers = nn.Sequential(*list(old_network.layers.children())[:2])\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(20, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_two_layers(x)\n",
    "        return self.new_layers(x)\n",
    "\n",
    "old_network = OldNetwork()\n",
    "new_network = NewNetwork(old_network)\n",
    "```\n",
    "\n",
    "In this example, we create a new network `NewNetwork` that takes an instance of `OldNetwork` as input. The new network uses the first two layers from the old network and adds new layers with a different architecture.\n",
    "\n",
    "3. Saving the network architecture and parameters:\n",
    "\n",
    "To save the network architecture and parameters, you can use a combination of Python's `pickle` module and PyTorch's `state_dict()` method. The `state_dict()` method returns a dictionary containing the model's parameters, while `pickle` can be used to save and load the model architecture.\n",
    "\n",
    "However, using `pickle` to save the entire model (including architecture) can be problematic due to potential compatibility issues between different PyTorch versions or environments. A safer approach is to save only the model's `state_dict` and recreate the model architecture manually when loading the model.\n",
    "\n",
    "```python\n",
    "# Save the model parameters\n",
    "torch.save(new_network.state_dict(), 'new_network_parameters.pt')\n",
    "\n",
    "# Load the model parameters\n",
    "loaded_parameters = torch.load('new_network_parameters.pt')\n",
    "\n",
    "# Recreate the model architecture and load the parameters\n",
    "old_network = OldNetwork()\n",
    "new_network = NewNetwork(old_network)\n",
    "new_network.load_state_dict(loaded_parameters)\n",
    "```\n",
    "\n",
    "Regarding restrictions on the architecture, it's essential to ensure that the model architecture is compatible with the saved parameters when loading the model. This means that the layers' dimensions, types, and order should match the saved parameters. If there are any mismatches, you may encounter errors when loading the parameters or during the forward pass of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

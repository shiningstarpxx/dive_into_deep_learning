{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have chosen an architecture and set our hyperparameters, we proceed to the training loop, where our goal is to find parameter values that minimize our loss function. After training, we will need these parameters in order to make future predictions. Additionally, we will sometimes wish to extract the parameters perhaps to reuse them in some other context, to save our model to disk so that it may be executed in other software, or for examination in the hope of gaining scientific understanding.\n",
    "\n",
    "when we move away from stacked architectures with standard layers, we will sometimes need to get into the weeds of declaring and manipulating parameters. In this section, we cover the following:\n",
    "\n",
    "1. Accessing parameters for debugging, diagnostics, and visualizations.\n",
    "\n",
    "2. Sharing parameters across different model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/miniconda3/envs/d2l-pytorch/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0537, -0.1868,  0.2111, -0.2906,  0.1202, -0.0968, -0.0236,  0.1317]])),\n",
       "             ('bias', tensor([-0.3029]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that this fully connected layer contains two parameters, corresponding to that layer’s weights and biases, respectively.\n",
    "\n",
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([-0.3029]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code extracts the bias from the second neural network layer, which returns a parameter class instance, and further accesses that parameter’s value.\n",
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state.\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# . Below we demonstrate accessing the parameters of all layers.\n",
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "# You might wonder, when parameters are tied what happens to the gradients? Since the model parameters contain gradients, \n",
    "# the gradients of the second hidden layer and the third hidden layer are added together during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accessing the parameters of various layers in NestMLP:\n",
    "\n",
    "Assuming that the NestMLP model is defined as follows (as in Section 6.1):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.net(x))\n",
    "```\n",
    "\n",
    "To access the parameters of various layers, you can use the `named_parameters()` method:\n",
    "\n",
    "```python\n",
    "nest_mlp = NestMLP()\n",
    "for name, param in nest_mlp.named_parameters():\n",
    "    print(name, param.size())\n",
    "```\n",
    "\n",
    "2. Constructing an MLP containing a shared parameter layer and observing the model parameters and gradients:\n",
    "\n",
    "```python\n",
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, shared_layer):\n",
    "        super().__init__()\n",
    "        self.shared_layer = shared_layer\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.shared_layer(x))\n",
    "\n",
    "shared_layer = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU())\n",
    "mlp1 = SharedMLP(shared_layer)\n",
    "mlp2 = SharedMLP(shared_layer)\n",
    "\n",
    "# Train the MLPs and observe the parameters and gradients\n",
    "# ...\n",
    "```\n",
    "\n",
    "During the training process, you can observe the model parameters and gradients of each layer using the following code snippet:\n",
    "\n",
    "```python\n",
    "for model in [mlp1, mlp2]:\n",
    "    print(f\"Model: {model}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(\"Value:\", param.data)\n",
    "        print(\"Gradient:\", param.grad)\n",
    "    print()\n",
    "```\n",
    "\n",
    "3. Why is sharing parameters a good idea?\n",
    "\n",
    "Sharing parameters is a good idea in certain scenarios, such as when you have multiple parts of your model that perform the same operation and learn the same features. By sharing parameters, you can:\n",
    "\n",
    "- Reduce the total number of parameters in your model, which can help prevent overfitting.\n",
    "- Save memory and computation during training, as the shared layers only need to be updated once.\n",
    "- Enable transfer learning, where a pre-trained model can be fine-tuned for a new task with a smaller dataset.\n",
    "\n",
    "However, it's important to note that sharing parameters is not always the best approach. It can limit the model's capacity to learn distinct features in different parts of the model. You should carefully consider the problem and the architecture of your model before deciding to share parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
